# app/api/v1/endpoints/chat.py
"""
๐ค ููุทุฉ ููุงูุฉ ุงูุฏุฑุฏุดุฉ - ูุณุฎุฉ ุงูุฅูุชุงุฌ
ุจุญุซ ุฐูู + ูุนุงูุฌุฉ ุญููููุฉ ูููููุงุช ูุงูุตูุฑ (OCR/PDF/Docs)
"""
import re
import os
import uuid
import time
import json
from collections import Counter
from fastapi import APIRouter, HTTPException, Form, UploadFile, File
from typing import Optional
from app.db.mysql_conn import execute_query
from app.utils.file_processor import extract_text_from_file, summarize_extracted_text

router = APIRouter()

UPLOAD_DIR = os.environ.get("UPLOAD_DIR", "uploads")

# ุงูุชุฏุงุฏุงุช ูููุงุช ุงูุตูุฑ ุงููุฏุนููุฉ
IMAGE_EXTENSIONS = {"jpg", "jpeg", "png", "gif", "bmp", "webp", "tiff", "tif", "svg"}

# ===== ูููุงุช ุงูุชููู ุงูุนุฑุจูุฉ (ููุณูุนุฉ) =====
STOP_WORDS = {
    "ูู", "ูู", "ุนูู", "ุฅูู", "ุงูู", "ุนู", "ูุน", "ูุฐุง", "ูุฐู", "ุฐูู", "ุชูู",
    "ุงูุชู", "ุงูุฐู", "ุงููุฐุงู", "ุงููุชุงู", "ุงูุฐูู", "ุงููุงุชู", "ุงูููุงุชู",
    "ูู", "ูู", "ูู", "ูู", "ุฃูุง", "ูุญู", "ุฃูุช", "ุฃูุชู", "ุฃูุชู",
    "ูุงู", "ูุงูุช", "ูููู", "ุชููู", "ูุงููุง", "ููุณ", "ููุณุช",
    "ูุง", "ูุง", "ูู", "ูู", "ูุฏ", "ุณูู", "ุณุฃ", "ุณูููู",
    "ู", "ุฃู", "ุซู", "ู", "ููู", "ุจู", "ุฅู", "ุฃู", "ุงู",
    "ูู", "ุจุนุถ", "ุฃู", "ููู", "ุฃูู", "ูุชู", "ููุงุฐุง", "ูุงุฐุง",
    "ูู", "ุฅุฐุง", "ุนูุฏ", "ุนูุฏูุง", "ุญุชู", "ููุฐ", "ุจูู",
    "ููุง", "ููุงู", "ุงูุขู", "ุฃูุถุงู", "ุฃูุถุง", "ุฌุฏุงู", "ุฌุฏุง", "ููุท",
    "ุงู", "ูู", "ุจุงู", "ุบูุฑ", "ุจุฏูู", "ุญูู", "ุฎูุงู",
    "ูุง", "ูู", "ูู", "ูู", "ููุง", "ููุง", "ููู", "ููู",
    "ุนู", "ุงูู", "ุนูู", "ููู", "ูููุง", "ููู", "ูููุง",
    "ุฃูู", "ุฃููุง", "ุฅูู", "ุฅููุง", "ูุฃู", "ูุงู",
    "ููุง", "ูุซู", "ูุซูุง", "ุญูุซ", "ุจุนุฏ", "ูุจู", "ููู", "ุชุญุช",
    "ูุฐู", "ูุงุฐุง", "ูุงุฐู", "ุฐุง", "ุฏุง", "ุฏู", "ุงููู",
    "ุดู", "ุงูุด", "ูุด", "ูููุง", "ููุด", "ููู",
    "ูุนูู", "ุทูุจ", "ุฎูุงุต", "ุจุณ", "ููุงู", "ุจุฑุถู", "ุจุฑุถู",
    "ูููู", "ูููู", "ูุงุฒู", "ุนุดุงู", "ุนูุดุงู",
    "is", "the", "a", "an", "and", "or", "what", "how", "why",
    "can", "do", "does", "are", "am", "was", "were", "be", "to", "of",
    "in", "on", "at", "for", "with", "about", "it", "this", "that",
}

# ===== ูุฑุงุฏูุงุช ุงูุฃุณุฆูุฉ =====
QUESTION_SYNONYMS = {
    "explain": ["ุงุดุฑุญ", "ูุถุญ", "ูุณุฑ", "ุจูู", "ุญุฏุซูู", "explain"],
    "what": ["ูุง", "ูุงูู", "ูุงูู", "ุงูุด", "ุดู", "ูุด", "ูุงุฐุง", "ุนุฑู", "ุนุฑููู", "what"],
    "how": ["ููู", "ููููุฉ", "ุทุฑููุฉ", "ุงุฒุงู", "ุดููู", "how"],
    "why": ["ููุงุฐุง", "ููุด", "ููู", "ููู", "why"],
    "difference": ["ูุฑู", "ุงุฎุชูุงู", "ููุงุฑูุฉ", "ูุงุฑู", "difference", "compare", "vs"],
    "tell": ["ุงุฎุจุฑูู", "ุฎุจุฑูู", "ููู", "ูููู", "ุงุญูููู", "tell"],
    "know": ["ุงุนุฑู", "ุนุงูุฒ", "ุงุจุบู", "ุงุจู", "ุงุฑูุฏ", "know"],
}


def normalize_arabic(text):
    """ุชุทุจูุน ุดุงูู ูููุต ุงูุนุฑุจู"""
    if not text:
        return ""
    # ุฅุฒุงูุฉ ุงูุชุดููู
    text = re.sub(r'[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06DC\u06DF-\u06E8\u06EA-\u06ED]', '', text)
    # ุชูุญูุฏ ุงูููุฒุงุช
    text = re.sub(r'[ุฅุฃุขูฑ]', 'ุง', text)
    text = re.sub(r'[ุค]', 'ู', text)
    text = re.sub(r'[ุฆ]', 'ู', text)
    # ุชูุญูุฏ ุญุฑูู
    text = text.replace('ุฉ', 'ู')
    text = text.replace('ู', 'ู')
    text = text.replace('ู', 'ู')
    # ุฅุฒุงูุฉ ุงูุชุทููู
    text = re.sub(r'ู+', '', text)
    # ุฅุฒุงูุฉ ุชูุฑุงุฑุงุช ุงูุฃุญุฑู
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    # ุญุฐู "ุงู" ุงูุชุนุฑูู ูู ุจุฏุงูุฉ ุงููููุงุช
    words = text.split()
    cleaned = []
    for w in words:
        w = w.strip()
        if w.startswith('ุงู') and len(w) > 3:
            cleaned.append(w[2:])
            cleaned.append(w)
        else:
            cleaned.append(w)
    return ' '.join(cleaned).strip()


def extract_keywords(text):
    """ุงุณุชุฎุฑุงุฌ ูููุงุช ููุชุงุญูุฉ ุฐููุฉ"""
    normalized = normalize_arabic(text.lower())
    clean = re.sub(r'[^\w\s]', ' ', normalized)
    words = [w.strip() for w in clean.split() if w.strip()]
    keywords = [w for w in words if w not in STOP_WORDS and len(w) > 1] # ุชู ุชูููู ุงูุญุฏ ูู 1

    stems = set()
    for kw in keywords:
        stems.add(kw)
        for prefix in ['ุงู', 'ูุงู', 'ุจุงู', 'ูู', 'ูุงู', 'ูุงู', 'ููู']:
            if kw.startswith(prefix) and len(kw) > len(prefix) + 2:
                stems.add(kw[len(prefix):])
        for suffix in ['ุงุช', 'ูู', 'ูู', 'ุงู', 'ูุง', 'ูู', 'ูุฉ', 'ูู', 'ูู', 'ูุง']:
            if kw.endswith(suffix) and len(kw) > len(suffix) + 2:
                stems.add(kw[:-len(suffix)])

    return list(stems) if stems else words[:5]


def fuzzy_match(word1, word2):
    """ูุทุงุจูุฉ ุถุจุงุจูุฉ ูุญุณููุฉ"""
    if not word1 or not word2:
        return 0.0
    w1 = normalize_arabic(word1.lower())
    w2 = normalize_arabic(word2.lower())

    if w1 == w2:
        return 1.0

    if w1 in w2 or w2 in w1:
        shorter = min(len(w1), len(w2))
        longer = max(len(w1), len(w2))
        return shorter / longer

    set1 = set(w1)
    set2 = set(w2)
    intersection = set1 & set2
    union = set1 | set2
    if not union:
        return 0.0
    jaccard = len(intersection) / len(union)

    common_prefix = 0
    for c1, c2 in zip(w1, w2):
        if c1 == c2:
            common_prefix += 1
        else:
            break
    prefix_bonus = common_prefix / max(len(w1), len(w2)) * 0.3

    return min(jaccard + prefix_bonus, 1.0)


def score_chunk(query, content):
    """ุญุณุงุจ ุตูุฉ ุงููุทุนุฉ ุจุงูุณุคุงู"""
    if not content or not query:
        return 0.0

    query_norm = normalize_arabic(query.lower())
    content_norm = normalize_arabic(content.lower())

    q_clean = re.sub(r'[^\w\s]', ' ', query_norm)
    c_clean = re.sub(r'[^\w\s]', ' ', content_norm)
    q_words = [w for w in q_clean.split() if w not in STOP_WORDS and len(w) > 1]
    c_words = [w for w in c_clean.split() if len(w) > 1]
    c_set = set(c_words)

    if not q_words:
        return 0.0

    exact_matches = 0
    fuzzy_matches = 0
    for qw in q_words:
        if qw in c_set:
            exact_matches += 1
        else:
            best_fuzzy = max((fuzzy_match(qw, cw) for cw in c_words), default=0)
            if best_fuzzy > 0.6:
                fuzzy_matches += best_fuzzy

    keyword_score = (exact_matches + fuzzy_matches * 0.7) / len(q_words)

    phrase_score = 0.0
    if query_norm in content_norm:
        phrase_score = 1.0
    else:
        for i in range(len(q_words) - 2):
            trigram = ' '.join(q_words[i:i+3])
            if trigram in c_clean:
                phrase_score = 0.6
                break

    qa_score = 0.0
    qa_patterns = [
        r'ุณุคุงู\s*[:๏ผุ?]\s*(.*?)(?:ุฌูุงุจ|ุงุฌุงุจู|ุงูุงุฌุงุจู|ุงูุฌูุงุจ)\s*[:๏ผ]\s*(.*?)(?=ุณุคุงู|$)',
        r'ุณ\s*[:๏ผ]\s*(.*?)(?:ุฌ|ุฌูุงุจ)\s*[:๏ผ]\s*(.*?)(?=ุณ\s*[:๏ผ]|$)',
    ]
    for pattern in qa_patterns:
        for q_text, a_text in re.findall(pattern, content, re.DOTALL):
            q_norm_inner = normalize_arabic(q_text.lower().strip())
            q_inner_words = [w for w in re.sub(r'[^\w\s]', ' ', q_norm_inner).split()
                            if w not in STOP_WORDS and len(w) > 1]
            if not q_inner_words:
                continue

            match_count = 0
            for qw in q_words:
                for qiw in q_inner_words:
                    if fuzzy_match(qw, qiw) > 0.55:
                        match_count += 1
                        break
            
            ratio = match_count / max(len(q_words), 1)
            if ratio > qa_score:
                qa_score = ratio * 1.5

    tf_counter = Counter(c_words)
    total_words = max(len(c_words), 1)
    tf_score = sum(tf_counter.get(kw, 0) for kw in q_words) / total_words

    topic_bonus = 0.0
    for group, synonyms in QUESTION_SYNONYMS.items():
        q_has = any(s in query_norm for s in synonyms)
        c_has = any(s in content_norm for s in synonyms)
        if q_has and c_has:
            topic_bonus = 0.1
            break

    final = (
        keyword_score * 0.25 +
        phrase_score * 0.15 +
        qa_score * 0.30 +
        tf_score * 0.15 +
        topic_bonus * 0.15
    )

    return round(min(final, 1.0), 4)


def find_direct_answer(query, chunks, context_text=""):
    """
    ุงูุจุญุซ ุนู ุฅุฌุงุจุฉ ูุจุงุดุฑุฉ ูู ุงููุทุน ุฃู ุงูุณูุงู ุงูุฅุถุงูู (ุงููููุงุช)
    """
    all_content = [chunk.get("content", "") for chunk in chunks]
    if context_text:
        all_content.append(context_text)

    query_norm = normalize_arabic(query.lower())
    q_clean = re.sub(r'[^\w\s]', ' ', query_norm)
    q_words = [w for w in q_clean.split() if w not in STOP_WORDS and len(w) > 1]

    if not q_words:
        return None

    best_answer = None
    best_score = 0

    qa_patterns = [
        r'ุณุคุงู\s*[:๏ผุ?]\s*(.*?)\s*(?:ุฌูุงุจ|ุงุฌุงุจู|ุงูุงุฌุงุจู|ุงูุฌูุงุจ)\s*[:๏ผ]\s*(.*?)(?=ุณุคุงู|$)',
        r'ุณ\s*[:๏ผ]\s*(.*?)(?:ุฌ|ุฌูุงุจ)\s*[:๏ผ]\s*(.*?)(?=ุณ\s*[:๏ผ]|$)',
    ]

    for content in all_content:
        for pattern in qa_patterns:
            for q_text, a_text in re.findall(pattern, content, re.DOTALL):
                q_stored = normalize_arabic(q_text.lower().strip())
                q_stored_words = [w for w in re.sub(r'[^\w\s]', ' ', q_stored).split()
                                 if w not in STOP_WORDS and len(w) > 1]

                if not q_stored_words:
                    continue

                match_count = 0
                for qw in q_words:
                    for sw in q_stored_words:
                        if fuzzy_match(qw, sw) > 0.5:
                            match_count += 1
                            break

                score = match_count / max(len(q_words), len(q_stored_words))

                if score > best_score and score > 0.25:
                    best_score = score
                    best_answer = a_text.strip()

    return best_answer


def build_smart_answer(question, top_chunks, file_context=None, memory_context=""):
    """ุจูุงุก ุฅุฌุงุจุฉ ุฐููุฉ ูู ุงููุทุน ูุณูุงู ุงููููุงุช ูุงูุฐุงูุฑุฉ"""

    # ูููุงุช ุชุดูุฑ ุฅูู ุฃู ุงููุณุชุฎุฏู ูุณุฃู ุนู ุงูููู/ุงูุตูุฑุฉ ูุจุงุดุฑุฉ
    _q = question.lower()
    DESCRIBE_WORDS = {"ูุณุฑ", "ูุตู", "ุงูุตู", "ุญูู", "ุงูุฑุง", "ุงูุฑุฃ", "ูุงุฐุง", "ูุด", "describe",
                      "analyze", "read", "tell", "ุงุฎุจุฑูู", "ุฎุจุฑูู", "ูุง", "show"}
    is_file_query = file_context and any(w in _q for w in DESCRIBE_WORDS)

    # ===== ุงุณุชุฌุงุจุฉ ูุฎุตุตุฉ ููุตูุฑ (ูุนูุฏ ุฏุงุฆูุงู info ุงูุตูุฑุฉ + ุชูุถูุญ OCR) =====
    if file_context and file_context.get("type") == "image":
        ft   = file_context.get("text", "")
        fname = file_context.get("filename", "")
        if ft and "๐ท" in ft:
            # has metadata
            return (
                f"๐ **ุงูุตูุฑุฉ ุงููุฑููุฉ:** {fname}\n\n"
                f"{ft}\n\n"
                "---\n"
                "โ๏ธ **ููุงุญุธุฉ:** ููุฑุงุกุฉ ุงููุตูุต ุงูููุชูุจุฉ ุฏุงุฎู ุงูุตูุฑุฉ ุจุฏูุฉ ูุญุชุงุฌ ุงููุธุงู ุฅูู ุฃุฏุงุฉ OCR "
                "(pytesseract + Tesseract). ูููู ุชุซุจูุชูุง ุจุงูุฃูุฑ:\n"
                "```\npip install Pillow pytesseract\n```\n"
                "๐ก ุฅุฐุง ูุงู ุณุคุงูู ุนู ููุถูุน ูุนูู ูุฑุชุจุท ุจุงูุตูุฑุฉุ ุงูุชุจ ุงูุณุคุงู ูุตูุงู ูุณุฃุจุญุซ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ."
            )
        else:
            return (
                f"๐ **ุชู ุงุณุชูุงู ุงูุตูุฑุฉ:** {fname}\n\n"
                "โ๏ธ ูู ูุชููู ุงููุธุงู ูู ุงุณุชุฎุฑุงุฌ ุงููุต ูููุง (OCR ุบูุฑ ูุซุจุช).\n\n"
                "๐ก ุงูุชุจ ุณุคุงูู ูุตูุงู ูุณุฃุจุญุซ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุนู ุงููุนูููุงุช ุงููุชุนููุฉ ุจุงูุตูุฑุฉ."
            )

    # ===== ููู ูุฑูู (ุบูุฑ ุตูุฑุฉ) ุจุฏูู ูุต =====
    if file_context and file_context.get("type") == "attached" and not file_context.get("text"):
        fname = file_context.get("filename", "")
        return (
            f"๐ **ุชู ุงุณุชูุงู ุงูููู:** {fname}\n\n"
            "โ๏ธ ูู ูุชููู ุงููุธุงู ูู ุงุณุชุฎุฑุงุฌ ุงููุต ููู. ุชุฃูุฏ ูู:\n"
            "- ุฃู ุงูููู ูุญุชูู ุนูู ูุต ูุงุจู ูููุณุฎ (ูููุณ ุตูุฑ ููุณูุญุฉ)\n"
            "- ุฃู ุงูููุชุจุงุช ุงููุทููุจุฉ ูุซุจุชุฉ: `pip install PyPDF2 pdfminer.six`\n\n"
            "๐ก ุงูุชุจ ุณุคุงูู ูุตูุงู ููุจุญุซ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ."
        )

    # 1. ุฅุฌุงุจุฉ ูุจุงุดุฑุฉ (ุชุดูู ุงูุฐุงูุฑุฉ ูุงูููู)
    all_extra = " ".join(filter(None, [memory_context, str(file_context or "")]))
    direct = find_direct_answer(question, top_chunks, all_extra)
    if direct:
        return direct

    # 2. ุชุฌููุน ูู ุงููููุงุช ุงููุฑููุฉ (ุงูุฃููููุฉ ููุง ุนูุฏ ุงูุฃุณุฆูุฉ ุนู ุงูููู)
    parts = []
    
    if file_context and file_context.get("text"):
        file_text = file_context["text"]
        
        # ุฅุฐุง ุงูููู ุตุบูุฑุ ุงุณุชุฎุฏูู ููู
        if len(file_text) < 500:
             parts.append(f"ูู ุงูููู ุงููุฑูู:\n{file_text}")
        else:
             # ุงูุจุญุซ ูู ุงูููู ุนู ุงูุฅุฌุงุจุฉ
             file_chunks = [file_text[i:i+500] for i in range(0, len(file_text), 400)]
             best_file_chunk = max(file_chunks, key=lambda c: score_chunk(question, c), default="")
             if score_chunk(question, best_file_chunk) > 0.1:
                 parts.append(f"ูู ุงูููู ุงููุฑูู:\n...{best_file_chunk}...")
             else:
                 parts.append(f"ููุฎุต ุงูููู ุงููุฑูู:\n{file_text[:300]}...")

        # ุฅุฐุง ูุงู ุงูุณุคุงู ุนู ุงูููู ูุจุงุดุฑุฉุ ูุง ุชูุถู ูุชุงุฆุฌ KB ุบูุฑ ุฐุงุช ุตูุฉ
        if is_file_query:
            return parts[0] if parts else "ูู ูุชููู ุงููุธุงู ูู ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงูููู."

    # 3. ุชุฌููุน ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ
    relevant = [c for c in top_chunks if c.get("_score", 0) > 0.05]
    for c in relevant[:3]:
        content = c.get("content", "").strip()
        qa_match = re.search(r'ุฌูุงุจ\s*[:๏ผ]\s*(.*?)(?=ุณุคุงู|$)', content, re.DOTALL)
        if qa_match:
            parts.append(qa_match.group(1).strip())
        else:
            parts.append(content)

    if not parts:
        # 4. ุงูุฐุงูุฑุฉ โ ุฅุฐุง ูุงู ุงูุณุคุงู ูุชุนูู ุจูุญุงุฏุซุฉ ุณุงุจูุฉ
        if memory_context:
            mem_score = score_chunk(question, memory_context)
            if mem_score > 0.1:
                return f"ุจูุงุกู ุนูู ูุญุงุฏุซุชูุง ุงูุณุงุจูุฉ:\n\n{memory_context[:600]}"
        if file_context and file_context.get("text"):
            ft = file_context["text"]
            return f"๐ ุงุณุชููุช ุงูููู ุงููุฑูู.\n\nูุนูููุงุช ุงูููู:\n{ft}"
        return "ุนุฐุฑุงูุ ูู ุฃุฌุฏ ูุนูููุงุช ูุงููุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ ููุฅุฌุงุจุฉ ุนูู ุณุคุงูู. ููููู ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู."

    if len(parts) == 1:
        return parts[0]
    else:
        combined = "\n\n".join(parts)
        return f"ุจูุงุกู ุนูู ุงููุนูููุงุช ุงููุชุงุญุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ:\n\n{combined}"


@router.post("/chat")
def chat(question: str = Form(...), thread_id: Optional[str] = Form(None)):
    """ุฏุฑุฏุดุฉ ูุตูุฉ ููุท"""
    # ... (ููุณ ุงูููุทู ุงูุณุงุจูุ ููู ุชู ูููู ูุฏุงูุฉ ูุดุชุฑูุฉ ููุงุฎุชุตุงุฑ) ...
    return process_chat_request(question, thread_id, None)


@router.post("/chat/json")
def chat_json(request: dict):
    question = request.get("question", "").strip()
    thread_id = request.get("thread_id")
    if not question:
        raise HTTPException(status_code=400, detail="ุงูุณุคุงู ูุทููุจ")
    return process_chat_request(question, thread_id, None)


@router.post("/chat/with-image")
async def chat_with_image(
    question: str = Form(...),
    thread_id: Optional[str] = Form(None),
    image: UploadFile = File(None),  # ูุฏ ูููู ููู ุฃู ุตูุฑุฉ
):
    """ุฏุฑุฏุดุฉ ูุน ููู (ุตูุฑุฉุ PDFุ ูุณุชูุฏ)"""
    file_info = None
    if image:
        try:
            content = await image.read()
            
            # ูุนุงูุฌุฉ ุงูููู ูุงุณุชุฎุฑุงุฌ ุงููุต (ุชูุฑูุฑ ุงุณู ุงูููู ูุงุณุชุฎุฑุงุฌ ุงูุงูุชุฏุงุฏ ุจุดูู ุตุญูุญ)
            file_result = extract_text_from_file(image.filename, image.content_type, content)
            
            # ุญูุธ ุงูููู
            os.makedirs(UPLOAD_DIR, exist_ok=True)
            safe_name = f"{uuid.uuid4()}_{image.filename}"
            file_path = os.path.join(UPLOAD_DIR, safe_name)
            with open(file_path, "wb") as f:
                f.write(content)
                
            # ุญูุธ ูู DB
            file_id = str(uuid.uuid4())
            try:
                execute_query(
                    "INSERT INTO ai_files (id, filename, mime_type, file_size, file_path, extracted_text) VALUES (%s, %s, %s, %s, %s, %s)",
                    (file_id, image.filename, image.content_type, len(content), file_path, file_result.get("text", "")[:5000])
                )
            except:
                pass
            
            file_info = {
                "file_id": file_id,
                "filename": image.filename,
                "text": file_result.get("text", ""),
                "type": file_result.get("metadata", {}).get("type", "unknown")
            }
            
        except Exception as e:
            print(f"File process error: {e}")

    return process_chat_request(question, thread_id, file_info)


def process_chat_request(question: str, thread_id: Optional[str], file_context: Optional[dict]):
    """ููุทู ุงูุฏุฑุฏุดุฉ ุงููุดุชุฑู"""
    start_time = time.time()
    question = question.strip() if question else ""

    # ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงูููู ุงููุถููู ูู ุงูุณุคุงู (ูู PHP two-step upload)
    if file_context is None and "[ูุญุชูู ุงูููู ุงููุฑูู" in question:
        try:
            marker = "[ูุญุชูู ุงูููู ุงููุฑูู"
            m_start = question.index(marker)
            clean_question = question[:m_start].strip()
            block = question[m_start:]
            # ุงุณุชุฎุฑุงุฌ ุงุณู ุงูููู
            fname = "ููู"
            if "'" in block:
                try:
                    fname = block[block.index("'") + 1 : block.index("':\n")]
                except Exception:
                    pass
            # ุงุณุชุฎุฑุงุฌ ุงููุญุชูู
            content = ""
            if "':\n" in block:
                content = block[block.index("':\n") + 3:].rstrip("]").strip()
            # ุชุญุฏูุฏ ููุน ุงูููู
            ftype = "uploaded"
            if fname.lower().split(".")[-1] in IMAGE_EXTENSIONS:
                ftype = "image"
            file_context = {"filename": fname, "text": content, "type": ftype}
            question = clean_question or question
        except Exception:
            pass

    # ูุฐูู: marker ูุฎุชุตุฑ "[ุงูููู ุงููุฑูู: ...]" โ ููุณุชุฎุฏู ุนูุฏูุง ูุง ููุณุชุฎุฑุฌ ูุต ูู ุงูููู
    if file_context is None and "[ุงูููู ุงููุฑูู:" in question:
        try:
            marker = "[ุงูููู ุงููุฑูู:"
            m_start = question.index(marker)
            clean_question = question[:m_start].strip()
            block = question[m_start + len(marker):]
            fname = block.rstrip("]").strip()
            ftype = "image" if fname.lower().split(".")[-1] in IMAGE_EXTENSIONS else "attached"
            file_context = {"filename": fname, "text": "", "type": ftype}
            question = clean_question or question
        except Exception:
            pass

    if not question and not file_context:
        raise HTTPException(status_code=400, detail="ุงูุณุคุงู ุฃู ุงูููู ูุทููุจ")

    # 1. Thread Management
    is_new_thread = False
    if not thread_id:
        thread_id = str(uuid.uuid4())
        try:
            execute_query("INSERT INTO ai_threads (id, title, metadata) VALUES (%s, %s, %s)", (thread_id, question[:80], '{}'))
            is_new_thread = True
        except: pass

    # 2. Load thread memory (for continuing conversations)
    memory_context = ""
    if thread_id and not is_new_thread:
        try:
            mem_rows = execute_query(
                "SELECT key_facts FROM ai_thread_memory WHERE thread_id = %s",
                (thread_id,)
            ) or []
            if mem_rows:
                key_facts = json.loads(mem_rows[0].get("key_facts") or "[]")
                if key_facts:
                    memory_context = "ุณูุงู ุงููุญุงุฏุซุฉ:\n" + "\n".join([
                        f"ุณ: {f['q']}\nุฌ: {f['a'][:150]}" for f in key_facts[-5:]
                    ])
        except Exception as me:
            print(f"Memory load error: {me}")
    
    # 2. Search
    keywords = extract_keywords(question)
    raw_chunks = []
    
    # Search logic (same as before)
    if keywords:
        conditions = " OR ".join(["content LIKE %s" for _ in keywords])
        params = tuple(f"%{kw}%" for kw in keywords)
        results = execute_query(f"SELECT id, content FROM ai_document_chunks WHERE {conditions} LIMIT 50", params) or []
        raw_chunks.extend(results)
        
    for chunk in raw_chunks:
        chunk["_score"] = score_chunk(question, chunk.get("content", ""))
    
    raw_chunks.sort(key=lambda x: x.get("_score", 0), reverse=True)
    top_chunks = raw_chunks[:10]

    # 3. Build Answer (with file context and memory)
    answer = build_smart_answer(question, top_chunks, file_context, memory_context)

    # 4. Save & Return
    latency_ms = int((time.time() - start_time) * 1000)
    asst_msg_id = str(uuid.uuid4())  # define early to avoid NameError if save fails

    # Save messages...
    try:
        user_msg_id = str(uuid.uuid4())
        
        # User message
        content_to_save = question
        if file_context:
            content_to_save += f"\n[ูุฑูู: {file_context['filename']}]"
            
        execute_query(
            "INSERT INTO ai_messages (id, thread_id, role, content, language, tokens) VALUES (%s, %s, %s, %s, %s, %s)",
            (user_msg_id, thread_id, 'user', content_to_save, 'ar', len(question.split()))
        )
        
        # Assistant message
        execute_query(
            "INSERT INTO ai_messages (id, thread_id, role, content, model, tokens, latency_ms, language) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)",
            (asst_msg_id, thread_id, 'assistant', answer, 'local-rag-v1', len(answer.split()), latency_ms, 'ar')
        )
        
        # Link file if exists (only when file_id is available)
        if file_context and file_context.get('file_id'):
             execute_query("INSERT INTO ai_message_files (message_id, file_id) VALUES (%s, %s)", (user_msg_id, file_context['file_id']))
             
    except Exception as e:
        print(f"Save error: {e}")

    # 5. Update thread memory (store Q&A for future context)
    try:
        mem_rows = execute_query(
            "SELECT key_facts FROM ai_thread_memory WHERE thread_id = %s",
            (thread_id,)
        ) or []
        key_facts = []
        if mem_rows:
            try:
                key_facts = json.loads(mem_rows[0].get("key_facts") or "[]")
            except Exception:
                pass
        key_facts.append({"q": question[:200], "a": answer[:300]})
        key_facts = key_facts[-10:]  # keep last 10 turns
        kf_json = json.dumps(key_facts, ensure_ascii=False)
        summary = f"ุขุฎุฑ ุณุคุงู: {question[:100]}"
        execute_query(
            "INSERT INTO ai_thread_memory (thread_id, summary, key_facts) VALUES (%s, %s, %s) "
            "ON DUPLICATE KEY UPDATE summary=%s, key_facts=%s, last_updated=NOW()",
            (thread_id, summary, kf_json, summary, kf_json)
        )
    except Exception as me:
        print(f"Memory save error: {me}")

    sources_used = [c for c in top_chunks if c.get("_score", 0) > 0]
    return {
        "status": "ok",
        "thread_id": thread_id,
        "message_id": asst_msg_id,
        "answer": answer,
        "sources": [{"chunk_id": c["id"], "content": c["content"][:100], "score": c["_score"]} for c in sources_used[:3]],
        "metadata": {
            "latency_ms": latency_ms,
            "input_tokens": len(question.split()),
            "output_tokens": len(answer.split()),
            "sources_found": len(sources_used),
            "model": "local-rag-v1",
            "has_file": bool(file_context),
            "has_memory": bool(memory_context),
            "file_info": file_context.get('filename') if file_context else None,
        }
    }
